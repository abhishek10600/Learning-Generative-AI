{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nRKK_mC7QD1S",
        "outputId": "6668f911-4f4b-4782-b869-fa82058444c2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting arxiv\n",
            "  Downloading arxiv-2.1.3-py3-none-any.whl.metadata (6.1 kB)\n",
            "Collecting feedparser~=6.0.10 (from arxiv)\n",
            "  Downloading feedparser-6.0.11-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: requests~=2.32.0 in /usr/local/lib/python3.10/dist-packages (from arxiv) (2.32.3)\n",
            "Collecting sgmllib3k (from feedparser~=6.0.10->arxiv)\n",
            "  Downloading sgmllib3k-1.0.0.tar.gz (5.8 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests~=2.32.0->arxiv) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests~=2.32.0->arxiv) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests~=2.32.0->arxiv) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests~=2.32.0->arxiv) (2024.8.30)\n",
            "Downloading arxiv-2.1.3-py3-none-any.whl (11 kB)\n",
            "Downloading feedparser-6.0.11-py3-none-any.whl (81 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.3/81.3 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: sgmllib3k\n",
            "  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sgmllib3k: filename=sgmllib3k-1.0.0-py3-none-any.whl size=6047 sha256=d298700030449ea86c37bc71a7252d21672274299ad7528a7a021dfd7bca8faf\n",
            "  Stored in directory: /root/.cache/pip/wheels/f0/69/93/a47e9d621be168e9e33c7ce60524393c0b92ae83cf6c6e89c5\n",
            "Successfully built sgmllib3k\n",
            "Installing collected packages: sgmllib3k, feedparser, arxiv\n",
            "Successfully installed arxiv-2.1.3 feedparser-6.0.11 sgmllib3k-1.0.0\n"
          ]
        }
      ],
      "source": [
        "!pip install arxiv"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_TPOiu8VTbAQ",
        "outputId": "554a853d-79b5-4193-de79-80ee36156d8c"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.46.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.26.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.6)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.10.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import arxiv\n",
        "import pandas as pd\n",
        "from transformers import pipeline"
      ],
      "metadata": {
        "id": "0kiywKp0Qdse"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Query to fetch AI-related papers\n",
        "\n",
        "query = \"ai OR artificial intelligence OR machine learning OR deep learning\"\n",
        "search = arxiv.Search(query=query, max_results=10, sort_by=arxiv.SortCriterion.SubmittedDate)\n",
        "\n",
        "#Fetch papers\n",
        "papers = []\n",
        "for result in search.results():\n",
        "  papers.append({\n",
        "      \"published\":result.published,\n",
        "      \"title\":result.title,\n",
        "      \"abstract\":result.summary,\n",
        "      \"categories\":result.categories,\n",
        "\n",
        "  })\n",
        "\n",
        "# print(papers[0])\n",
        "\n",
        "# Convert to DataFrame\n",
        "df = pd.DataFrame(papers)\n",
        "\n",
        "pd.set_option(\"display.max_colwidth\", None)\n",
        "df.head(10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "S6vF25RyQlGW",
        "outputId": "36964a28-d57b-478b-8b70-a1be63bc7b28"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-10-aa08cd028c69>:8: DeprecationWarning: The 'Search.results' method is deprecated, use 'Client.results' instead\n",
            "  for result in search.results():\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                  published  \\\n",
              "0 2024-11-15 18:59:51+00:00   \n",
              "1 2024-11-15 18:57:39+00:00   \n",
              "2 2024-11-15 18:56:01+00:00   \n",
              "3 2024-11-15 18:56:00+00:00   \n",
              "4 2024-11-15 18:53:08+00:00   \n",
              "5 2024-11-15 18:50:53+00:00   \n",
              "6 2024-11-15 18:43:29+00:00   \n",
              "7 2024-11-15 18:42:48+00:00   \n",
              "8 2024-11-15 18:38:18+00:00   \n",
              "9 2024-11-15 18:35:00+00:00   \n",
              "\n",
              "                                                                                                                    title  \\\n",
              "0                                                         VeriGraph: Scene Graphs for Execution Verifiable Robot Planning   \n",
              "1                                              MARS: Unleashing the Power of Variance Reduction for Training Large Models   \n",
              "2   Mitigating Hallucination in Multimodal Large Language Model via Hallucination-targeted Direct Preference Optimization   \n",
              "3                                                        The Spatial Complexity of Optical Computing and How to Reduce It   \n",
              "4  Mitigating Parameter Degeneracy using Joint Conditional Diffusion Model for WECC Composite Load Model in Power Systems   \n",
              "5                                                                Private Counterfactual Retrieval With Immutable Features   \n",
              "6                                      Back to Supervision: Boosting Word Boundary Detection through Frame Classification   \n",
              "7         Evaluating Creativity and Deception in Large Language Models: A Simulation Framework for Multi-Agent Balderdash   \n",
              "8                                                              Multiscale Dubuc: A New Similarity Measure for Time Series   \n",
              "9                                                            Towards Automatic Evaluation of Task-Oriented Dialogue Flows   \n",
              "\n",
              "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              abstract  \\\n",
              "0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             Recent advancements in vision-language models (VLMs) offer potential for\\nrobot task planning, but challenges remain due to VLMs' tendency to generate\\nincorrect action sequences. To address these limitations, we propose VeriGraph,\\na novel framework that integrates VLMs for robotic planning while verifying\\naction feasibility. VeriGraph employs scene graphs as an intermediate\\nrepresentation, capturing key objects and spatial relationships to improve plan\\nverification and refinement. The system generates a scene graph from input\\nimages and uses it to iteratively check and correct action sequences generated\\nby an LLM-based task planner, ensuring constraints are respected and actions\\nare executable. Our approach significantly enhances task completion rates\\nacross diverse manipulation scenarios, outperforming baseline methods by 58%\\nfor language-based tasks and 30% for image-based tasks.   \n",
              "1                                                                                                                                                                                                                                                                                                                                  Training deep neural networks--and more recently, large models--demands\\nefficient and scalable optimizers. Adaptive gradient algorithms like Adam,\\nAdamW, and their variants have been central to this task. Despite the\\ndevelopment of numerous variance reduction algorithms in the past decade aimed\\nat accelerating stochastic optimization in both convex and nonconvex settings,\\nvariance reduction has not found widespread success in training deep neural\\nnetworks or large language models. Consequently, it has remained a less favored\\napproach in modern AI. In this paper, to unleash the power of variance\\nreduction for efficient training of large models, we propose a unified\\noptimization framework, MARS (Make vAriance Reduction Shine), which reconciles\\npreconditioned gradient methods with variance reduction via a scaled stochastic\\nrecursive momentum technique. Within our framework, we introduce three\\ninstances of MARS that leverage preconditioned gradient updates based on AdamW,\\nLion, and Shampoo, respectively. We also draw a connection between our\\nalgorithms and existing optimizers. Experimental results on training GPT-2\\nmodels indicate that MARS consistently outperforms AdamW by a large margin.   \n",
              "2                                                                                                                                                                                                                                                                                                                                                                                                       Multimodal Large Language Models (MLLMs) are known to hallucinate, which\\nlimits their practical applications. Recent works have attempted to apply\\nDirect Preference Optimization (DPO) to enhance the performance of MLLMs, but\\nhave shown inconsistent improvements in mitigating hallucinations. To address\\nthis issue more effectively, we introduce Hallucination-targeted Direct\\nPreference Optimization (HDPO) to reduce hallucinations in MLLMs. Unlike\\nprevious approaches, our method tackles hallucinations from their diverse forms\\nand causes. Specifically, we develop three types of preference pair data\\ntargeting the following causes of MLLM hallucinations: (1) insufficient visual\\ncapabilities, (2) long context generation, and (3) multimodal conflicts.\\nExperimental results demonstrate that our method achieves superior performance\\nacross multiple hallucination evaluation datasets, surpassing most\\nstate-of-the-art (SOTA) methods and highlighting the potential of our approach.\\nAblation studies and in-depth analyses further confirm the effectiveness of our\\nmethod and suggest the potential for further improvements through scaling up.   \n",
              "3                                                                          Similar to algorithms, which consume time and memory to run, hardware\\nrequires resources to function. For devices processing physical waves,\\nimplementing operations needs sufficient \"space,\" as dictated by wave physics.\\nHow much space is needed to perform a certain function is a fundamental\\nquestion in optics, with recent research addressing it for given mathematical\\noperations, but not for more general computing tasks, e.g., classification.\\nInspired by computational complexity theory, we study the \"spatial complexity\"\\nof optical computing systems in terms of scaling laws - specifically, how their\\nphysical dimensions must scale as the dimension of the mathematical operation\\nincreases - and propose a new paradigm for designing optical computing systems:\\nspace-efficient neuromorphic optics, based on structural sparsity constraints\\nand neural pruning methods motivated by wave physics (notably, the concept of\\n\"overlapping nonlocality\"). On two mainstream platforms, free-space optics and\\non-chip integrated photonics, our methods demonstrate substantial size\\nreductions (to 1%-10% the size of conventional designs) with minimal compromise\\non performance. Our theoretical and computational results reveal a trend of\\ndiminishing returns on accuracy as structure dimensions increase, providing a\\nnew perspective for interpreting and approaching the ultimate limits of optical\\ncomputing - a balanced trade-off between device size and accuracy.   \n",
              "4                           Data-driven modeling for dynamic systems has gained widespread attention in\\nrecent years. Its inverse formulation, parameter estimation, aims to infer the\\ninherent model parameters from observations. However, parameter degeneracy,\\nwhere different combinations of parameters yield the same observable output,\\nposes a critical barrier to accurately and uniquely identifying model\\nparameters. In the context of WECC composite load model (CLM) in power systems,\\nutility practitioners have observed that CLM parameters carefully selected for\\none fault event may not perform satisfactorily in another fault. Here, we\\ninnovate a joint conditional diffusion model-based inverse problem solver\\n(JCDI), that incorporates a joint conditioning architecture with simultaneous\\ninputs of multi-event observations to improve parameter generalizability.\\nSimulation studies on the WECC CLM show that the proposed JCDI effectively\\nreduces uncertainties of degenerate parameters, thus the parameter estimation\\nerror is decreased by 42.1% compared to a single-event learning scheme. This\\nenables the model to achieve high accuracy in predicting power trajectories\\nunder different fault events, including electronic load tripping and motor\\nstalling, outperforming standard deep reinforcement learning and supervised\\nlearning approaches. We anticipate this work will contribute to mitigating\\nparameter degeneracy in system dynamics, providing a general parameter\\nestimation framework across various scientific domains.   \n",
              "5                                                                                                                                                                                                                                                                                                                                                                  In a classification task, counterfactual explanations provide the minimum\\nchange needed for an input to be classified into a favorable class. We consider\\nthe problem of privately retrieving the exact closest counterfactual from a\\ndatabase of accepted samples while enforcing that certain features of the input\\nsample cannot be changed, i.e., they are \\emph{immutable}. An applicant (user)\\nwhose feature vector is rejected by a machine learning model wants to retrieve\\nthe sample closest to them in the database without altering a private subset of\\ntheir features, which constitutes the immutable set. While doing this, the user\\nshould keep their feature vector, immutable set and the resulting\\ncounterfactual index information-theoretically private from the institution. We\\nrefer to this as immutable private counterfactual retrieval (I-PCR) problem\\nwhich generalizes PCR to a more practical setting. In this paper, we propose\\ntwo I-PCR schemes by leveraging techniques from private information retrieval\\n(PIR) and characterize their communication costs. Further, we quantify the\\ninformation that the user learns about the database and compare it for the\\nproposed schemes.   \n",
              "6                                                                                                                                                                                                                                                                                                                                            Speech segmentation at both word and phoneme levels is crucial for various\\nspeech processing tasks. It significantly aids in extracting meaningful units\\nfrom an utterance, thus enabling the generation of discrete elements. In this\\nwork we propose a model-agnostic framework to perform word boundary detection\\nin a supervised manner also employing a labels augmentation technique and an\\noutput-frame selection strategy. We trained and tested on the Buckeye dataset\\nand only tested on TIMIT one, using state-of-the-art encoder models, including\\npre-trained solutions (Wav2Vec 2.0 and HuBERT), as well as convolutional and\\nconvolutional recurrent networks. Our method, with the HuBERT encoder,\\nsurpasses the performance of other state-of-the-art architectures, whether\\ntrained in supervised or self-supervised settings on the same datasets.\\nSpecifically, we achieved F-values of 0.8427 on the Buckeye dataset and 0.7436\\non the TIMIT dataset, along with R-values of 0.8489 and 0.7807, respectively.\\nThese results establish a new state-of-the-art for both datasets. Beyond the\\nimmediate task, our approach offers a robust and efficient preprocessing method\\nfor future research in audio tokenization.   \n",
              "7                                                                                                                                                                                                                                                                              Large Language Models (LLMs) have shown impressive capabilities in complex\\ntasks and interactive environments, yet their creativity remains underexplored.\\nThis paper introduces a simulation framework utilizing the game Balderdash to\\nevaluate both the creativity and logical reasoning of LLMs. In Balderdash,\\nplayers generate fictitious definitions for obscure terms to deceive others\\nwhile identifying correct definitions. Our framework enables multiple LLM\\nagents to participate in this game, assessing their ability to produce\\nplausible definitions and strategize based on game rules and history. We\\nimplemented a centralized game engine featuring various LLMs as participants\\nand a judge LLM to evaluate semantic equivalence. Through a series of\\nexperiments, we analyzed the performance of different LLMs, examining metrics\\nsuch as True Definition Ratio, Deception Ratio, and Correct Guess Ratio. The\\nresults provide insights into the creative and deceptive capabilities of LLMs,\\nhighlighting their strengths and areas for improvement. Specifically, the study\\nreveals that infrequent vocabulary in LLMs' input leads to poor reasoning on\\ngame rules and historical context\\n(https://github.com/ParsaHejabi/Simulation-Framework-for-Multi-Agent-Balderdash).   \n",
              "8  Quantifying similarities between time series in a meaningful way remains a\\nchallenge in time series analysis, despite many advances in the field. Most\\nreal-world solutions still rely on a few popular measures, such as Euclidean\\nDistance (EuD), Longest Common Subsequence (LCSS), and Dynamic Time Warping\\n(DTW). The strengths and weaknesses of these measures have been studied\\nextensively, and incremental improvements have been proposed. In this study,\\nhowever, we present a different similarity measure that fuses the notion of\\nDubuc's variation from fractal analysis with the Intersection-over-Union (IoU)\\nmeasure which is widely used in object recognition (also known as the Jaccard\\nIndex). In this proof-of-concept paper, we introduce the Multiscale Dubuc\\nDistance (MDD) measure and prove that it is a metric, possessing desirable\\nproperties such as the triangle inequality. We use 95 datasets from the UCR\\nTime Series Classification Archive to compare MDD's performance with EuD, LCSS,\\nand DTW. Our experiments show that MDD's overall success, without any\\ncase-specific customization, is comparable to DTW with optimized window sizes\\nper dataset. We also highlight several datasets where MDD's performance\\nimproves significantly when its single parameter is customized. This\\ncustomization serves as a powerful tool for gauging MDD's sensitivity to noise.\\nLastly, we show that MDD's running time is linear in the length of the time\\nseries, which is crucial for real-world applications involving very large\\ndatasets.   \n",
              "9                                                                                                                                                                                                                                                                                                                                  Task-oriented dialogue systems rely on predefined conversation schemes\\n(dialogue flows) often represented as directed acyclic graphs. These flows can\\nbe manually designed or automatically generated from previously recorded\\nconversations. Due to variations in domain expertise or reliance on different\\nsets of prior conversations, these dialogue flows can manifest in significantly\\ndifferent graph structures. Despite their importance, there is no standard\\nmethod for evaluating the quality of dialogue flows. We introduce FuDGE (Fuzzy\\nDialogue-Graph Edit Distance), a novel metric that evaluates dialogue flows by\\nassessing their structural complexity and representational coverage of the\\nconversation data. FuDGE measures how well individual conversations align with\\na flow and, consequently, how well a set of conversations is represented by the\\nflow overall. Through extensive experiments on manually configured flows and\\nflows generated by automated techniques, we demonstrate the effectiveness of\\nFuDGE and its evaluation framework. By standardizing and optimizing dialogue\\nflows, FuDGE enables conversational designers and automated techniques to\\nachieve higher levels of efficiency and automation.   \n",
              "\n",
              "                                categories  \n",
              "0                           [cs.RO, cs.AI]  \n",
              "1                [cs.LG, math.OC, stat.ML]  \n",
              "2             [cs.CL, cs.AI, cs.CV, cs.MM]  \n",
              "3           [physics.optics, cs.ET, cs.LG]  \n",
              "4                  [cs.AI, cs.SY, eess.SY]  \n",
              "5  [cs.IT, cs.CR, cs.LG, eess.SP, math.IT]  \n",
              "6                                  [cs.LG]  \n",
              "7                           [cs.MA, cs.AI]  \n",
              "8                                  [cs.LG]  \n",
              "9                           [cs.CL, cs.AI]  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-9212a640-2789-44c4-bbf8-92dc45225fa1\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>published</th>\n",
              "      <th>title</th>\n",
              "      <th>abstract</th>\n",
              "      <th>categories</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2024-11-15 18:59:51+00:00</td>\n",
              "      <td>VeriGraph: Scene Graphs for Execution Verifiable Robot Planning</td>\n",
              "      <td>Recent advancements in vision-language models (VLMs) offer potential for\\nrobot task planning, but challenges remain due to VLMs' tendency to generate\\nincorrect action sequences. To address these limitations, we propose VeriGraph,\\na novel framework that integrates VLMs for robotic planning while verifying\\naction feasibility. VeriGraph employs scene graphs as an intermediate\\nrepresentation, capturing key objects and spatial relationships to improve plan\\nverification and refinement. The system generates a scene graph from input\\nimages and uses it to iteratively check and correct action sequences generated\\nby an LLM-based task planner, ensuring constraints are respected and actions\\nare executable. Our approach significantly enhances task completion rates\\nacross diverse manipulation scenarios, outperforming baseline methods by 58%\\nfor language-based tasks and 30% for image-based tasks.</td>\n",
              "      <td>[cs.RO, cs.AI]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2024-11-15 18:57:39+00:00</td>\n",
              "      <td>MARS: Unleashing the Power of Variance Reduction for Training Large Models</td>\n",
              "      <td>Training deep neural networks--and more recently, large models--demands\\nefficient and scalable optimizers. Adaptive gradient algorithms like Adam,\\nAdamW, and their variants have been central to this task. Despite the\\ndevelopment of numerous variance reduction algorithms in the past decade aimed\\nat accelerating stochastic optimization in both convex and nonconvex settings,\\nvariance reduction has not found widespread success in training deep neural\\nnetworks or large language models. Consequently, it has remained a less favored\\napproach in modern AI. In this paper, to unleash the power of variance\\nreduction for efficient training of large models, we propose a unified\\noptimization framework, MARS (Make vAriance Reduction Shine), which reconciles\\npreconditioned gradient methods with variance reduction via a scaled stochastic\\nrecursive momentum technique. Within our framework, we introduce three\\ninstances of MARS that leverage preconditioned gradient updates based on AdamW,\\nLion, and Shampoo, respectively. We also draw a connection between our\\nalgorithms and existing optimizers. Experimental results on training GPT-2\\nmodels indicate that MARS consistently outperforms AdamW by a large margin.</td>\n",
              "      <td>[cs.LG, math.OC, stat.ML]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2024-11-15 18:56:01+00:00</td>\n",
              "      <td>Mitigating Hallucination in Multimodal Large Language Model via Hallucination-targeted Direct Preference Optimization</td>\n",
              "      <td>Multimodal Large Language Models (MLLMs) are known to hallucinate, which\\nlimits their practical applications. Recent works have attempted to apply\\nDirect Preference Optimization (DPO) to enhance the performance of MLLMs, but\\nhave shown inconsistent improvements in mitigating hallucinations. To address\\nthis issue more effectively, we introduce Hallucination-targeted Direct\\nPreference Optimization (HDPO) to reduce hallucinations in MLLMs. Unlike\\nprevious approaches, our method tackles hallucinations from their diverse forms\\nand causes. Specifically, we develop three types of preference pair data\\ntargeting the following causes of MLLM hallucinations: (1) insufficient visual\\ncapabilities, (2) long context generation, and (3) multimodal conflicts.\\nExperimental results demonstrate that our method achieves superior performance\\nacross multiple hallucination evaluation datasets, surpassing most\\nstate-of-the-art (SOTA) methods and highlighting the potential of our approach.\\nAblation studies and in-depth analyses further confirm the effectiveness of our\\nmethod and suggest the potential for further improvements through scaling up.</td>\n",
              "      <td>[cs.CL, cs.AI, cs.CV, cs.MM]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2024-11-15 18:56:00+00:00</td>\n",
              "      <td>The Spatial Complexity of Optical Computing and How to Reduce It</td>\n",
              "      <td>Similar to algorithms, which consume time and memory to run, hardware\\nrequires resources to function. For devices processing physical waves,\\nimplementing operations needs sufficient \"space,\" as dictated by wave physics.\\nHow much space is needed to perform a certain function is a fundamental\\nquestion in optics, with recent research addressing it for given mathematical\\noperations, but not for more general computing tasks, e.g., classification.\\nInspired by computational complexity theory, we study the \"spatial complexity\"\\nof optical computing systems in terms of scaling laws - specifically, how their\\nphysical dimensions must scale as the dimension of the mathematical operation\\nincreases - and propose a new paradigm for designing optical computing systems:\\nspace-efficient neuromorphic optics, based on structural sparsity constraints\\nand neural pruning methods motivated by wave physics (notably, the concept of\\n\"overlapping nonlocality\"). On two mainstream platforms, free-space optics and\\non-chip integrated photonics, our methods demonstrate substantial size\\nreductions (to 1%-10% the size of conventional designs) with minimal compromise\\non performance. Our theoretical and computational results reveal a trend of\\ndiminishing returns on accuracy as structure dimensions increase, providing a\\nnew perspective for interpreting and approaching the ultimate limits of optical\\ncomputing - a balanced trade-off between device size and accuracy.</td>\n",
              "      <td>[physics.optics, cs.ET, cs.LG]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2024-11-15 18:53:08+00:00</td>\n",
              "      <td>Mitigating Parameter Degeneracy using Joint Conditional Diffusion Model for WECC Composite Load Model in Power Systems</td>\n",
              "      <td>Data-driven modeling for dynamic systems has gained widespread attention in\\nrecent years. Its inverse formulation, parameter estimation, aims to infer the\\ninherent model parameters from observations. However, parameter degeneracy,\\nwhere different combinations of parameters yield the same observable output,\\nposes a critical barrier to accurately and uniquely identifying model\\nparameters. In the context of WECC composite load model (CLM) in power systems,\\nutility practitioners have observed that CLM parameters carefully selected for\\none fault event may not perform satisfactorily in another fault. Here, we\\ninnovate a joint conditional diffusion model-based inverse problem solver\\n(JCDI), that incorporates a joint conditioning architecture with simultaneous\\ninputs of multi-event observations to improve parameter generalizability.\\nSimulation studies on the WECC CLM show that the proposed JCDI effectively\\nreduces uncertainties of degenerate parameters, thus the parameter estimation\\nerror is decreased by 42.1% compared to a single-event learning scheme. This\\nenables the model to achieve high accuracy in predicting power trajectories\\nunder different fault events, including electronic load tripping and motor\\nstalling, outperforming standard deep reinforcement learning and supervised\\nlearning approaches. We anticipate this work will contribute to mitigating\\nparameter degeneracy in system dynamics, providing a general parameter\\nestimation framework across various scientific domains.</td>\n",
              "      <td>[cs.AI, cs.SY, eess.SY]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>2024-11-15 18:50:53+00:00</td>\n",
              "      <td>Private Counterfactual Retrieval With Immutable Features</td>\n",
              "      <td>In a classification task, counterfactual explanations provide the minimum\\nchange needed for an input to be classified into a favorable class. We consider\\nthe problem of privately retrieving the exact closest counterfactual from a\\ndatabase of accepted samples while enforcing that certain features of the input\\nsample cannot be changed, i.e., they are \\emph{immutable}. An applicant (user)\\nwhose feature vector is rejected by a machine learning model wants to retrieve\\nthe sample closest to them in the database without altering a private subset of\\ntheir features, which constitutes the immutable set. While doing this, the user\\nshould keep their feature vector, immutable set and the resulting\\ncounterfactual index information-theoretically private from the institution. We\\nrefer to this as immutable private counterfactual retrieval (I-PCR) problem\\nwhich generalizes PCR to a more practical setting. In this paper, we propose\\ntwo I-PCR schemes by leveraging techniques from private information retrieval\\n(PIR) and characterize their communication costs. Further, we quantify the\\ninformation that the user learns about the database and compare it for the\\nproposed schemes.</td>\n",
              "      <td>[cs.IT, cs.CR, cs.LG, eess.SP, math.IT]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>2024-11-15 18:43:29+00:00</td>\n",
              "      <td>Back to Supervision: Boosting Word Boundary Detection through Frame Classification</td>\n",
              "      <td>Speech segmentation at both word and phoneme levels is crucial for various\\nspeech processing tasks. It significantly aids in extracting meaningful units\\nfrom an utterance, thus enabling the generation of discrete elements. In this\\nwork we propose a model-agnostic framework to perform word boundary detection\\nin a supervised manner also employing a labels augmentation technique and an\\noutput-frame selection strategy. We trained and tested on the Buckeye dataset\\nand only tested on TIMIT one, using state-of-the-art encoder models, including\\npre-trained solutions (Wav2Vec 2.0 and HuBERT), as well as convolutional and\\nconvolutional recurrent networks. Our method, with the HuBERT encoder,\\nsurpasses the performance of other state-of-the-art architectures, whether\\ntrained in supervised or self-supervised settings on the same datasets.\\nSpecifically, we achieved F-values of 0.8427 on the Buckeye dataset and 0.7436\\non the TIMIT dataset, along with R-values of 0.8489 and 0.7807, respectively.\\nThese results establish a new state-of-the-art for both datasets. Beyond the\\nimmediate task, our approach offers a robust and efficient preprocessing method\\nfor future research in audio tokenization.</td>\n",
              "      <td>[cs.LG]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>2024-11-15 18:42:48+00:00</td>\n",
              "      <td>Evaluating Creativity and Deception in Large Language Models: A Simulation Framework for Multi-Agent Balderdash</td>\n",
              "      <td>Large Language Models (LLMs) have shown impressive capabilities in complex\\ntasks and interactive environments, yet their creativity remains underexplored.\\nThis paper introduces a simulation framework utilizing the game Balderdash to\\nevaluate both the creativity and logical reasoning of LLMs. In Balderdash,\\nplayers generate fictitious definitions for obscure terms to deceive others\\nwhile identifying correct definitions. Our framework enables multiple LLM\\nagents to participate in this game, assessing their ability to produce\\nplausible definitions and strategize based on game rules and history. We\\nimplemented a centralized game engine featuring various LLMs as participants\\nand a judge LLM to evaluate semantic equivalence. Through a series of\\nexperiments, we analyzed the performance of different LLMs, examining metrics\\nsuch as True Definition Ratio, Deception Ratio, and Correct Guess Ratio. The\\nresults provide insights into the creative and deceptive capabilities of LLMs,\\nhighlighting their strengths and areas for improvement. Specifically, the study\\nreveals that infrequent vocabulary in LLMs' input leads to poor reasoning on\\ngame rules and historical context\\n(https://github.com/ParsaHejabi/Simulation-Framework-for-Multi-Agent-Balderdash).</td>\n",
              "      <td>[cs.MA, cs.AI]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>2024-11-15 18:38:18+00:00</td>\n",
              "      <td>Multiscale Dubuc: A New Similarity Measure for Time Series</td>\n",
              "      <td>Quantifying similarities between time series in a meaningful way remains a\\nchallenge in time series analysis, despite many advances in the field. Most\\nreal-world solutions still rely on a few popular measures, such as Euclidean\\nDistance (EuD), Longest Common Subsequence (LCSS), and Dynamic Time Warping\\n(DTW). The strengths and weaknesses of these measures have been studied\\nextensively, and incremental improvements have been proposed. In this study,\\nhowever, we present a different similarity measure that fuses the notion of\\nDubuc's variation from fractal analysis with the Intersection-over-Union (IoU)\\nmeasure which is widely used in object recognition (also known as the Jaccard\\nIndex). In this proof-of-concept paper, we introduce the Multiscale Dubuc\\nDistance (MDD) measure and prove that it is a metric, possessing desirable\\nproperties such as the triangle inequality. We use 95 datasets from the UCR\\nTime Series Classification Archive to compare MDD's performance with EuD, LCSS,\\nand DTW. Our experiments show that MDD's overall success, without any\\ncase-specific customization, is comparable to DTW with optimized window sizes\\nper dataset. We also highlight several datasets where MDD's performance\\nimproves significantly when its single parameter is customized. This\\ncustomization serves as a powerful tool for gauging MDD's sensitivity to noise.\\nLastly, we show that MDD's running time is linear in the length of the time\\nseries, which is crucial for real-world applications involving very large\\ndatasets.</td>\n",
              "      <td>[cs.LG]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>2024-11-15 18:35:00+00:00</td>\n",
              "      <td>Towards Automatic Evaluation of Task-Oriented Dialogue Flows</td>\n",
              "      <td>Task-oriented dialogue systems rely on predefined conversation schemes\\n(dialogue flows) often represented as directed acyclic graphs. These flows can\\nbe manually designed or automatically generated from previously recorded\\nconversations. Due to variations in domain expertise or reliance on different\\nsets of prior conversations, these dialogue flows can manifest in significantly\\ndifferent graph structures. Despite their importance, there is no standard\\nmethod for evaluating the quality of dialogue flows. We introduce FuDGE (Fuzzy\\nDialogue-Graph Edit Distance), a novel metric that evaluates dialogue flows by\\nassessing their structural complexity and representational coverage of the\\nconversation data. FuDGE measures how well individual conversations align with\\na flow and, consequently, how well a set of conversations is represented by the\\nflow overall. Through extensive experiments on manually configured flows and\\nflows generated by automated techniques, we demonstrate the effectiveness of\\nFuDGE and its evaluation framework. By standardizing and optimizing dialogue\\nflows, FuDGE enables conversational designers and automated techniques to\\nachieve higher levels of efficiency and automation.</td>\n",
              "      <td>[cs.CL, cs.AI]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-9212a640-2789-44c4-bbf8-92dc45225fa1')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-9212a640-2789-44c4-bbf8-92dc45225fa1 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-9212a640-2789-44c4-bbf8-92dc45225fa1');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-b6761a8c-3cc0-408b-ab49-07c70139eeb0\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-b6761a8c-3cc0-408b-ab49-07c70139eeb0')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-b6761a8c-3cc0-408b-ab49-07c70139eeb0 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 10,\n  \"fields\": [\n    {\n      \"column\": \"published\",\n      \"properties\": {\n        \"dtype\": \"date\",\n        \"min\": \"2024-11-15 18:35:00+00:00\",\n        \"max\": \"2024-11-15 18:59:51+00:00\",\n        \"num_unique_values\": 10,\n        \"samples\": [\n          \"2024-11-15 18:38:18+00:00\",\n          \"2024-11-15 18:57:39+00:00\",\n          \"2024-11-15 18:50:53+00:00\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"title\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 10,\n        \"samples\": [\n          \"Multiscale Dubuc: A New Similarity Measure for Time Series\",\n          \"MARS: Unleashing the Power of Variance Reduction for Training Large Models\",\n          \"Private Counterfactual Retrieval With Immutable Features\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"abstract\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 10,\n        \"samples\": [\n          \"Quantifying similarities between time series in a meaningful way remains a\\nchallenge in time series analysis, despite many advances in the field. Most\\nreal-world solutions still rely on a few popular measures, such as Euclidean\\nDistance (EuD), Longest Common Subsequence (LCSS), and Dynamic Time Warping\\n(DTW). The strengths and weaknesses of these measures have been studied\\nextensively, and incremental improvements have been proposed. In this study,\\nhowever, we present a different similarity measure that fuses the notion of\\nDubuc's variation from fractal analysis with the Intersection-over-Union (IoU)\\nmeasure which is widely used in object recognition (also known as the Jaccard\\nIndex). In this proof-of-concept paper, we introduce the Multiscale Dubuc\\nDistance (MDD) measure and prove that it is a metric, possessing desirable\\nproperties such as the triangle inequality. We use 95 datasets from the UCR\\nTime Series Classification Archive to compare MDD's performance with EuD, LCSS,\\nand DTW. Our experiments show that MDD's overall success, without any\\ncase-specific customization, is comparable to DTW with optimized window sizes\\nper dataset. We also highlight several datasets where MDD's performance\\nimproves significantly when its single parameter is customized. This\\ncustomization serves as a powerful tool for gauging MDD's sensitivity to noise.\\nLastly, we show that MDD's running time is linear in the length of the time\\nseries, which is crucial for real-world applications involving very large\\ndatasets.\",\n          \"Training deep neural networks--and more recently, large models--demands\\nefficient and scalable optimizers. Adaptive gradient algorithms like Adam,\\nAdamW, and their variants have been central to this task. Despite the\\ndevelopment of numerous variance reduction algorithms in the past decade aimed\\nat accelerating stochastic optimization in both convex and nonconvex settings,\\nvariance reduction has not found widespread success in training deep neural\\nnetworks or large language models. Consequently, it has remained a less favored\\napproach in modern AI. In this paper, to unleash the power of variance\\nreduction for efficient training of large models, we propose a unified\\noptimization framework, MARS (Make vAriance Reduction Shine), which reconciles\\npreconditioned gradient methods with variance reduction via a scaled stochastic\\nrecursive momentum technique. Within our framework, we introduce three\\ninstances of MARS that leverage preconditioned gradient updates based on AdamW,\\nLion, and Shampoo, respectively. We also draw a connection between our\\nalgorithms and existing optimizers. Experimental results on training GPT-2\\nmodels indicate that MARS consistently outperforms AdamW by a large margin.\",\n          \"In a classification task, counterfactual explanations provide the minimum\\nchange needed for an input to be classified into a favorable class. We consider\\nthe problem of privately retrieving the exact closest counterfactual from a\\ndatabase of accepted samples while enforcing that certain features of the input\\nsample cannot be changed, i.e., they are \\\\emph{immutable}. An applicant (user)\\nwhose feature vector is rejected by a machine learning model wants to retrieve\\nthe sample closest to them in the database without altering a private subset of\\ntheir features, which constitutes the immutable set. While doing this, the user\\nshould keep their feature vector, immutable set and the resulting\\ncounterfactual index information-theoretically private from the institution. We\\nrefer to this as immutable private counterfactual retrieval (I-PCR) problem\\nwhich generalizes PCR to a more practical setting. In this paper, we propose\\ntwo I-PCR schemes by leveraging techniques from private information retrieval\\n(PIR) and characterize their communication costs. Further, we quantify the\\ninformation that the user learns about the database and compare it for the\\nproposed schemes.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"categories\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example abstract from API\n",
        "\n",
        "abstract = df[\"abstract\"][2]\n",
        "\n",
        "summarizer = pipeline(\"summarization\", model = \"facebook/bart-large-cnn\")\n",
        "\n",
        "# Summarization\n",
        "summarization_result = summarizer(abstract)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NZfOldpSRFGH",
        "outputId": "70c4d662-3381-4462-d251-59fc205faf74"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "summarization_result[0][\"summary_text\"]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "PM5h3fnMTyeB",
        "outputId": "f9e1a1f3-f210-4ec2-f973-f09a60da5fd0"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Multimodal Large Language Models (MLLMs) are known to hallucinate. We introduce Hallucination-targeted Direct Preference Optimization (HDPO) to reduce hallucinations in MLLMs. Unlike previous approaches, our method tackles hallucinations from their diverse forms and causes.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "78d5j1nLW7wc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}